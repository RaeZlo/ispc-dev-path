# Transformación del Entorno Laboral e Impacto de la Automatización en el Mercado Laboral

Estos temas se abordan principalmente en relación con los **desafíos éticos** que surgen con las nuevas tecnologías, especialmente la automatización y la Inteligencia Artificial (IA).

* La **automatización y la inteligencia artificial pueden tener un impacto en el empleo y la igualdad económica**.
* Ante este potencial impacto, surge una pregunta clave: **cómo pueden las empresas y los gobiernos abordar estos efectos para garantizar un futuro laboral justo**.
* Por ejemplo, el caso de DALL.E 2, una IA generativa de imágenes, muestra que estas tecnologías podrían tener **usos comerciales, gubernamentales, militares, etc.**, lo que implica cambios en los tipos de herramientas y procesos utilizados en diversos entornos de trabajo.

# Responsabilidad Ética en la Implementación de IA

Este es un tema central y se presenta como un **desafío fundamental en la era digital**.
Junto con los beneficios de la tecnología, han surgido **desafíos éticos significativos**.

### Aspectos destacados:

- **Privacidad de datos**: la recopilación y uso de datos personales es una preocupación central, y su gestión adecuada es esencial para la confianza de los usuarios, planteando preguntas sobre quién controla los datos y cómo se usan. ZeroQ, por ejemplo, prioriza la privacidad de datos y cumple con regulaciones rigurosas.
- **Automatización y empleo** (como se mencionó anteriormente, esto es visto también como un desafío ético).
- **Sesgo en la Inteligencia Artificial**: los algoritmos pueden contener sesgos inherentes que causan discriminación (créditos, justicia, contratación).
- **Responsabilidad en la creación de contenido**: la difusión de información (incluida la falsa) plantea preocupaciones sobre la responsabilidad de las plataformas tecnológicas. ZeroQ busca garantizar que sus soluciones sean éticas a través de políticas y prácticas sólidas, incluyendo transparencia en la gestión de datos y responsabilidad social.

# El caso de DALL.E 2 como ejemplo de los desafíos éticos de la IA

* DALL.E 2 es un sistema de IA que crea imágenes a partir de texto.
* La compañía OpenAI, que lo desarrolló, ha reportado problemas de seguridad y ha desarrollado mitigaciones, pero también ha identificado **sesgos, estereotipos dañinos, falta o representación inadecuada de la población, contenido explícito y desinformación**.
* DALL.E 2 tiene el potencial de **dañar a individuos y grupos reforzando estereotipos, borrándolos o denigrándolos**, brindando un desempeño de baja calidad, dispar o sometiéndolos a indignidad.

### Ejemplos específicos de sesgos encontrados:

* Sobre-representación de **conceptos occidentales y personas de piel blanca**.
* Reproducción de **estereotipos de género**, asignando roles de menor jerarquía a personajes femeninos (ej: "auxiliar de vuelo", "asistente personal") mientras términos neutros como "CEO" o "abogado" generan personajes masculinos.
* Potencial de creación de **contenido "explícito"** (sexual, odio, violencia) a partir de sugerencias.

### Preguntas críticas que surgen:

* **¿Cómo evitar abusos o cuestionar el sentido de construir estas tecnologías?**
* ¿En qué medida la preocupación ética depende de la voluntad de las empresas?

> **Detectar los problemas no garantiza poder resolverlos fácilmente**, ya que el sesgo a menudo es inherente a los datos de entrenamiento.

* Quitar las fuentes de sesgo puede limitar la funcionalidad del sistema o producir otros problemas. Un ejemplo es que filtrar imágenes de desnudez para evitar contenido explícito reducía la cantidad de imágenes de mujeres en general, introduciendo un sesgo de representación.
* Entrenar sistemas con **millones de fotos y textos producidos por humanos y esperar corregir luego los sesgos aprendidos no parece la mejor manera de crear IA justa y equitativa**. Este modelo reproduce y amplifica los sesgos. La IA aprende patrones de los datos sin comprensión del mundo real ni curación previa de datos.
* La construcción de estos sistemas requiere **inversiones millonarias**. Se cuestiona el sentido de invertir cíclicamente en reentrenar un sistema que se sabe estará sesgado y es imposible depurar completamente.
* Existe una lógica donde los **usuarios terminan testeando el software**, reportando incidentes una vez en uso. No se pueden cuantificar los daños producidos por sesgos no encontrados o que no se pueden erradicar.

> Se menciona el **principio de precaución** como una figura interesante a analizar, que prevé medidas protectoras ante riesgos graves para la sociedad. Los resultados del informe de OpenAI evidencian la incertidumbre sobre el funcionamiento del sistema.

# Respuestas regulatorias y la responsabilidad de las empresas

* Ha habido un **gran movimiento global para delinear qué pueden o no hacer los sistemas de IA**. Han surgido iniciativas como la Recomendación de la Unesco o el borrador legislativo de la UE, además de manifiestos de diversos actores.
* La mayoría de propuestas coinciden en la necesidad de una **evaluación de riesgo rigurosa e integral** de los sistemas de IA.

### Sin embargo:

* **No está claro cómo llevar a cabo estas evaluaciones en la práctica**, qué instituciones estarían a cargo, ni se han definido estándares o certificaciones adecuadas, salvo avances en el borrador de la UE (que aún no rige y solo afectaría a sistemas dentro o adquiridos para la UE).
* Hasta el momento, **ninguna empresa o institución está obligada legal o moralmente a realizar evaluaciones específicas** relacionadas con la ética de la tecnología de IA, más allá de las de calidad productiva.

### Iniciativas destacadas:

* **OpenAI tomó la iniciativa de publicar su informe** antes de desplegar DALL.E 2, buscando hacer transparente el proceso y mostrar alineación con las expectativas sociales sobre los riesgos de sesgos y representaciones incorrectas.
* **ZeroQ** se compromete a abordar estos desafíos éticos en su desarrollo y operaciones, buscando ser **líderes éticos** y asegurar que la tecnología beneficie a la sociedad.
